{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "759SG4TyfbUn",
        "Zj-h4drXD-X9",
        "BY6yifxscfrx",
        "k_ewoagic5jc",
        "70StdqAZa9E9"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Maestría en Inteligencia Artificial Aplicada**\n",
        "##**Curso: Procesamiento de Lenguaje Natural (NLP)**\n",
        "###Tecnológico de Monterrey\n",
        "###Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "## **Adtividad de la Semana 02**\n",
        "###**Introducción al procesamiento de texto.**"
      ],
      "metadata": {
        "id": "759SG4TyfbUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta actividad deberás utilizar los datos del siguiente archivo que se encuentra en Canvas:\n",
        "\n",
        "MNA_NLP_semana_02_Actividad_datos.txt\n",
        "\n",
        "El archivo contiene comentarios en inglés sobre servicios de comida de la página de Yelp: https://www.yelp.com/ .\n",
        "\n",
        "Son mil comentarios y forman parte del conjunto de datos que se encuentra en el Machine Learning Repository de la UCI, llamado \"Sentiment Labelled Sentences\": https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#\n"
      ],
      "metadata": {
        "id": "6ue1YAKx3XDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 1. Cargamos los datos.**   "
      ],
      "metadata": {
        "id": "Zj-h4drXD-X9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargar los datos del archivo indicado y obtener una lista de longitud de 1000 strings/comentarios.\n",
        "\n",
        "Por el momento solamente requerimos las bibliotecas de Numpy y re, para el manejo de los arreglos y de las expresiones regulares en Python.\n",
        "\n",
        "En particular, no necesitarás en esta actividad la biblioteca de Pandas.\n",
        "\n",
        "###**NOTA: En esta actividad no debes importar nada más, con estas dos bibliotecas será *suficiente*.**"
      ],
      "metadata": {
        "id": "BY6yifxscfrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np    # importamos Numpy para el manejo de los arreglos.\n",
        "import re             # importamos re para el manejo de las expresiones regulares."
      ],
      "metadata": {
        "id": "OJ26dAfhdFnf"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta las siguientes instrucciones para cargar la información del achivo dado:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "with open('/content/drive/MyDrive/MNA_NLP_semana_02_Actividad_datos.txt',        # puedes actualizar la ruta a tu archivo, en dado caso.\n",
        "          mode='r',     # abrimos el archivo en modo lectura.\n",
        "          ) as f:\n",
        "    docs = f.readlines()    # separamos cada comentario por líneas\n",
        "\n",
        "f.close()  # ya que tenemos la información en la variable docs, cerramos el archivo"
      ],
      "metadata": {
        "id": "QHUmJyjDdGNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bd98f79-3e6d-4f6f-af9d-a149d5f8dc57"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs) == list   # Verifica que tu variable \"docs\" es una lista"
      ],
      "metadata": {
        "id": "L6WzrSrodG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ce4730-c840-4ab7-d663-d06c343553d8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)==1000  # verifica que la longitud de \"docs\" es de mil comentarios."
      ],
      "metadata": {
        "id": "QIK1u9WS2FtS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fa10b76-4c92-4ab3-f86a-6fc68ac4f93c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0:10]     # observa algunos de los primeros comentarios"
      ],
      "metadata": {
        "id": "9AMLIfQvJqNZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c43a7aa-38bb-42a0-bcb3-1356c6be03de"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Wow... Loved this place.\\n',\n",
              " 'Crust is not good.\\n',\n",
              " 'Not tasty and the texture was just nasty.\\n',\n",
              " 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.\\n',\n",
              " 'The selection on the menu was great and so were the prices.\\n',\n",
              " 'Now I am getting angry and I want my damn pho.\\n',\n",
              " \"Honeslty it didn't taste THAT fresh.)\\n\",\n",
              " 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.\\n',\n",
              " 'The fries were great too.\\n',\n",
              " 'A great touch.\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 2: sección de preguntas (regex).**   \n"
      ],
      "metadata": {
        "id": "k_ewoagic5jc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Instrucciones:**\n",
        "\n",
        "###**A continuación deberás contestar cada una de las preguntas que te piden usando expresiones regulares (regex).**\n",
        "\n",
        "###**Por el momento no hay restricción en cuanto al número de líneas de código que agregues, pero trata de incluir las mínimas posibles.**"
      ],
      "metadata": {
        "id": "X-eMJa3DFCIV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 1.**\n",
        "\n",
        "Busca y elimina todos los saltos de línea '\\n' que se encuentran al final de cada comentario.\n",
        "\n",
        "Una vez finalizado, imprime los primeros 10 comentarios del resultado obtenido.\n"
      ],
      "metadata": {
        "id": "78nJMemzn5a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar los saltos de línea al final de cada comentario usando strip()\n",
        "docs_limpios = [comentario.strip() for comentario in docs]"
      ],
      "metadata": {
        "id": "PwbYYIuZn8pE"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los primeros 10 comentarios\n",
        "print(docs_limpios[:10])"
      ],
      "metadata": {
        "id": "j-0qeh2Jn8l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fc67d2e-934f-47d3-f1cf-18f3bcee1df8"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wow... Loved this place.', 'Crust is not good.', 'Not tasty and the texture was just nasty.', 'Stopped by during the late May bank holiday off Rick Steve recommendation and loved it.', 'The selection on the menu was great and so were the prices.', 'Now I am getting angry and I want my damn pho.', \"Honeslty it didn't taste THAT fresh.)\", 'The potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer.', 'The fries were great too.', 'A great touch.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 2.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan con dos o más signos de admiración seguidos, por ejemplo \"!!!\".\n",
        "\n",
        "Debes imprimir tanto la palabra como la totalidad de signos de admiración que le siguen.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n",
        "\n"
      ],
      "metadata": {
        "id": "VWeKQC93ctEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Busca cualquier palabra seguida directamente de 2+ signos de admiración\n",
        "pattern = r'\\b\\w+(?:!{2,})'\n",
        "\n",
        "# Buscar todas las coincidencias\n",
        "resultados = re.findall(pattern, ' '.join(docs_limpios))"
      ],
      "metadata": {
        "id": "0p3kMXfddICc"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los resultados\n",
        "for resultado in resultados:\n",
        "    print(resultado)\n",
        "\n",
        "print(f'Número total de resultados: {len(resultados)}')"
      ],
      "metadata": {
        "id": "SPVM1MCWdH6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a5aa64-2ddf-467c-d730-a5f2b6cb4cb0"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Firehouse!!!!!\n",
            "APPETIZERS!!!\n",
            "amazing!!!\n",
            "buffet!!!\n",
            "good!!\n",
            "it!!!!\n",
            "DELICIOUS!!\n",
            "amazing!!\n",
            "shawarrrrrrma!!!!!!\n",
            "yucky!!!\n",
            "steak!!!!!\n",
            "delicious!!!\n",
            "far!!\n",
            "biscuits!!!\n",
            "dry!!\n",
            "disappointing!!!\n",
            "awesome!!\n",
            "Up!!\n",
            "FLY!!!!!!!!\n",
            "here!!!\n",
            "great!!!!!!!!!!!!!!\n",
            "packed!!\n",
            "otherwise!!\n",
            "amazing!!!!!!!!!!!!!!!!!!!\n",
            "style!!\n",
            "disappointed!!\n",
            "Número total de resultados: 26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 3.**  \n",
        "\n",
        "Busca e imprime todas las palabras que están escritas totalmente en mayúsculas. Cada coincidencia debe ser una sola palabra.\n",
        "\n",
        "Indica cuántas palabras encontraste.\n",
        "\n"
      ],
      "metadata": {
        "id": "-s3okBqL96TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Expresión regular para palabras en mayúsculas (al menos 2 letras para evitar \"A\", \"Y\", etc. si quieres)\n",
        "pattern = r'\\b[A-ZÁÉÍÓÚÑ]{2,}\\b'\n",
        "\n",
        "# Buscar coincidencias\n",
        "mayusculas = re.findall(pattern, texto)"
      ],
      "metadata": {
        "id": "yKHJkZKo_nW5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir las palabras encontradas\n",
        "for palabra in mayusculas:\n",
        "    print(palabra)\n",
        "\n",
        "# Mostrar el total\n",
        "print(f'Número total de palabras en mayúsculas: {len(mayusculas)}')"
      ],
      "metadata": {
        "id": "L3q08aq69sNn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2d2861-90c1-464f-9432-04cca52e2956"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "THAT\n",
            "APPETIZERS\n",
            "WILL\n",
            "NEVER\n",
            "EVER\n",
            "STEP\n",
            "FORWARD\n",
            "IN\n",
            "IT\n",
            "AGAIN\n",
            "LOVED\n",
            "AND\n",
            "REAL\n",
            "BITCHES\n",
            "NYC\n",
            "STALE\n",
            "DELICIOUS\n",
            "WORST\n",
            "EXPERIENCE\n",
            "EVER\n",
            "ALL\n",
            "BARGAIN\n",
            "TV\n",
            "NONE\n",
            "FREEZING\n",
            "AYCE\n",
            "FLAVOR\n",
            "NEVER\n",
            "BBQ\n",
            "UNREAL\n",
            "OMG\n",
            "BETTER\n",
            "BLAND\n",
            "RUDE\n",
            "INCONSIDERATE\n",
            "MANAGEMENT\n",
            "WILL\n",
            "NEVER\n",
            "EVER\n",
            "GO\n",
            "BACK\n",
            "AND\n",
            "HAVE\n",
            "TOLD\n",
            "MANY\n",
            "PEOPLE\n",
            "WHAT\n",
            "HAD\n",
            "HAPPENED\n",
            "TOTAL\n",
            "WASTE\n",
            "OF\n",
            "TIME\n",
            "FS\n",
            "AZ\n",
            "LOVED\n",
            "CONCLUSION\n",
            "BEST\n",
            "GO\n",
            "NOW\n",
            "GC\n",
            "AVOID\n",
            "THIS\n",
            "ESTABLISHMENT\n",
            "AN\n",
            "HOUR\n",
            "NASTY\n",
            "OMG\n",
            "NO\n",
            "BEST\n",
            "THE\n",
            "OWNERS\n",
            "REALLY\n",
            "REALLY\n",
            "PERFECT\n",
            "SCREAMS\n",
            "LEGIT\n",
            "MGM\n",
            "BEST\n",
            "FLY\n",
            "FLY\n",
            "FANTASTIC\n",
            "GREAT\n",
            "OK\n",
            "WAY\n",
            "MUST\n",
            "HAVE\n",
            "OK\n",
            "OVERPRICED\n",
            "BARE\n",
            "HANDS\n",
            "WEAK\n",
            "SHOULD\n",
            "RI\n",
            "VERY\n",
            "NOT\n",
            "Número total de palabras en mayúsculas: 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 4.**  \n",
        "\n",
        "Busca e imprime los comentarios en donde todos los caracteres alfabéticos (letras) están en mayúsculas.\n",
        "\n",
        "Cada coincidencia encontrada debe ser todo el comentario/enunciado.\n",
        "\n",
        "Indica cuántos resultados obtuviste.\n"
      ],
      "metadata": {
        "id": "GX8eYyDoMZma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "comentarios_mayus = []\n",
        "\n",
        "for comentario in docs_limpios:\n",
        "    # Tomamos solo las letras del comentario\n",
        "    letras = ''.join(c for c in comentario if c.isalpha())\n",
        "\n",
        "    # Verificamos si todas las letras están en mayúsculas\n",
        "    if letras and letras.isupper():\n",
        "        comentarios_mayus.append(comentario)"
      ],
      "metadata": {
        "id": "K8VuZxvTMYj6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los comentarios encontrados\n",
        "for c in comentarios_mayus:\n",
        "    print(c)\n",
        "\n",
        "# Imprimir cuántos se encontraron\n",
        "print(f'Número total de comentarios con letras solo en mayúsculas: {len(comentarios_mayus)}')"
      ],
      "metadata": {
        "id": "PmKgX7sCMcDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c796404-8f18-4381-9168-453fef63a6cc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DELICIOUS!!\n",
            "RUDE & INCONSIDERATE MANAGEMENT.\n",
            "WILL NEVER EVER GO BACK AND HAVE TOLD MANY PEOPLE WHAT HAD HAPPENED.\n",
            "TOTAL WASTE OF TIME.\n",
            "AVOID THIS ESTABLISHMENT!\n",
            "Número total de comentarios con letras solo en mayúsculas: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 5.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una vocal acentuada, del tipo á, é, í, ó, ú.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "a1i6qv7-McmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Buscar palabras que contengan al menos una vocal acentuada\n",
        "pattern = r'\\b\\w*[áéíóúÁÉÍÓÚ]\\w*\\b'\n",
        "\n",
        "# Buscar coincidencias\n",
        "palabras_acentuadas = re.findall(pattern, texto)"
      ],
      "metadata": {
        "id": "nZZ5zKUOMeGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir las palabras encontradas\n",
        "for palabra in palabras_acentuadas:\n",
        "    print(palabra)\n",
        "\n",
        "# Mostrar cuántas se encontraron\n",
        "print(f'Número total de palabras con vocal acentuada: {len(palabras_acentuadas)}')"
      ],
      "metadata": {
        "id": "l1mFvUEZMe8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 6.**  \n",
        "\n",
        "Busca e imprime todas las cantidades numéricas monetarias, enteras o con decimales, que inician con el símbolo $\\$$.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "ZmPiAI82Mfb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expresión regular para encontrar cantidades monetarias que comienzan con $\n",
        "pattern = r'\\$\\d+(?:,\\d{3})*(?:\\.\\d{1,2})?'\n",
        "\n",
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Buscar las cantidades monetarias\n",
        "cantidades_monetarias = re.findall(pattern, texto)"
      ],
      "metadata": {
        "id": "6vhe9-Y-MhL9"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir las cantidades monetarias encontradas\n",
        "if cantidades_monetarias:\n",
        "    for cantidad in cantidades_monetarias:\n",
        "        print(cantidad)\n",
        "else:\n",
        "    print(\"No se encontraron cantidades monetarias.\")\n",
        "\n",
        "# Indicar cuántos resultados se encontraron\n",
        "print(f\"\\nNúmero total de cantidades monetarias encontradas: {len(cantidades_monetarias)}\")"
      ],
      "metadata": {
        "id": "_t0a5xWDMhQ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "180e1c33-0f38-42bc-aa10-d0beb58f3970"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "$20\n",
            "$4.00\n",
            "$17\n",
            "$3\n",
            "$35\n",
            "$7.85\n",
            "$12\n",
            "$11.99\n",
            "\n",
            "Número total de cantidades monetarias encontradas: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 7.**  \n",
        "\n",
        "Busca e imprime todas las palabras que sean variantes de la palabra \"love\", sin importar si incluyen mayúsculas o minúsculas, o la manera en que esté conjugada o alguna otra variación que se haga con dicha palabra.\n",
        "\n",
        "Indica cuántos resultados obtuviste."
      ],
      "metadata": {
        "id": "2j-HpvhwMhq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Expresión regular para palabras que contienen \"love\" (ignorando mayúsculas)\n",
        "pattern = r'\\b\\w*love\\w*\\b'\n",
        "\n",
        "# Buscar coincidencias ignorando mayúsculas/minúsculas\n",
        "variantes_love = re.findall(pattern, texto, flags=re.IGNORECASE)\n"
      ],
      "metadata": {
        "id": "kqqyRChVMjol"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir resultados\n",
        "for palabra in variantes_love:\n",
        "    print(palabra)\n",
        "\n",
        "print(f\"Número total de variantes de 'love': {len(variantes_love)}\")"
      ],
      "metadata": {
        "id": "UXd0VQluMj_x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "667faae4-8264-4fa9-f90b-a0e0381436cd"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loved\n",
            "loved\n",
            "Loved\n",
            "love\n",
            "loves\n",
            "LOVED\n",
            "lovers\n",
            "love\n",
            "lovers\n",
            "Love\n",
            "loved\n",
            "loved\n",
            "love\n",
            "love\n",
            "love\n",
            "loved\n",
            "love\n",
            "loved\n",
            "Love\n",
            "LOVED\n",
            "love\n",
            "lovely\n",
            "love\n",
            "lovely\n",
            "love\n",
            "lover\n",
            "loved\n",
            "love\n",
            "love\n",
            "love\n",
            "love\n",
            "gloves\n",
            "love\n",
            "love\n",
            "love\n",
            "love\n",
            "Número total de variantes de 'love': 36\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 8.**  \n",
        "\n",
        "Busca e imprime todas las palabras, variantes de \"so\" y \"good\", que tengan dos o más \"o\" en \"so\" y 3 o más \"o\" en good.\n",
        "\n",
        "Indica cuántas encontraste.\n"
      ],
      "metadata": {
        "id": "Ctb-NTY3MkYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir los comentarios\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Buscar variantes de \"so\" con 2 o más 'o'\n",
        "pattern_so = r'\\bso{2,}\\b'\n",
        "\n",
        "# Buscar variantes de \"good\" con 3 o más 'o'\n",
        "pattern_good = r'\\bgo{3,}d\\b'\n",
        "\n",
        "# Encontrar coincidencias ignorando mayúsculas\n",
        "variantes_so = re.findall(pattern_so, texto, flags=re.IGNORECASE)\n",
        "variantes_good = re.findall(pattern_good, texto, flags=re.IGNORECASE)"
      ],
      "metadata": {
        "id": "A8Nf3B_cMlqg"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir resultados\n",
        "print(\"Variantes de 'so' (2+ o's):\")\n",
        "for palabra in variantes_so:\n",
        "    print(palabra)\n",
        "\n",
        "print(\"\\nVariantes de 'good' (3+ o's):\")\n",
        "for palabra in variantes_good:\n",
        "    print(palabra)\n",
        "\n",
        "# Totales\n",
        "print(f\"\\nTotal de variantes de 'so': {len(variantes_so)}\")\n",
        "print(f\"Total de variantes de 'good': {len(variantes_good)}\")"
      ],
      "metadata": {
        "id": "svS4-vvPMl6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b454c666-53bf-40b8-a0e3-93799dde49b9"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Variantes de 'so' (2+ o's):\n",
            "Sooooo\n",
            "soooo\n",
            "soooooo\n",
            "soooo\n",
            "\n",
            "Variantes de 'good' (3+ o's):\n",
            "\n",
            "Total de variantes de 'so': 4\n",
            "Total de variantes de 'good': 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 9.**  \n",
        "\n",
        "Busca e imprime todas las palabras que tengan una longitud mayor estrictamente a 10 caracteres alfabéticos.\n",
        "\n",
        "No se consideran los signos de puntuación o caracteres especiales en la longitud de estas cadenas, solo caracteres alfabéticos en mayúsculas o minúsculas.\n",
        "\n",
        "Indica la cantidad de palabras encontradas.\n"
      ],
      "metadata": {
        "id": "hkak1opjMmlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Buscar todas las palabras (pueden tener signos o números)\n",
        "todas_las_palabras = re.findall(r'\\b\\w+\\b', texto)\n",
        "\n",
        "# Filtrar solo las que tienen más de 10 letras (alfabéticas)\n",
        "palabras_largas = [palabra for palabra in todas_las_palabras if len([c for c in palabra if c.isalpha()]) > 10]"
      ],
      "metadata": {
        "id": "PYxdp3uhMoD0"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir resultados\n",
        "for palabra in palabras_largas:\n",
        "    print(palabra)\n",
        "\n",
        "# Total\n",
        "print(f\"\\nNúmero total de palabras con más de 10 letras: {len(palabras_largas)}\")"
      ],
      "metadata": {
        "id": "BR7e2F4FMof-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f86a031-f0f8-4ab3-b736-e052014c0328"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "recommendation\n",
            "recommended\n",
            "overwhelmed\n",
            "inexpensive\n",
            "establishment\n",
            "imaginative\n",
            "opportunity\n",
            "experiencing\n",
            "underwhelming\n",
            "relationship\n",
            "unsatisfying\n",
            "disappointing\n",
            "outrageously\n",
            "disappointing\n",
            "expectations\n",
            "restaurants\n",
            "suggestions\n",
            "disappointed\n",
            "considering\n",
            "Unfortunately\n",
            "immediately\n",
            "ingredients\n",
            "accommodations\n",
            "maintaining\n",
            "Interesting\n",
            "disrespected\n",
            "accordingly\n",
            "unbelievable\n",
            "cheeseburger\n",
            "descriptions\n",
            "inexpensive\n",
            "disappointed\n",
            "Veggitarian\n",
            "outstanding\n",
            "recommendation\n",
            "disappointed\n",
            "disappointed\n",
            "neighborhood\n",
            "disappointed\n",
            "corporation\n",
            "considering\n",
            "exceptional\n",
            "shawarrrrrrma\n",
            "disappointed\n",
            "vinaigrette\n",
            "immediately\n",
            "unbelievably\n",
            "replenished\n",
            "disappointed\n",
            "enthusiastic\n",
            "Outstanding\n",
            "comfortable\n",
            "interesting\n",
            "INCONSIDERATE\n",
            "considering\n",
            "transcendant\n",
            "disappointment\n",
            "disappointed\n",
            "disappointed\n",
            "overwhelmed\n",
            "professional\n",
            "Furthermore\n",
            "combination\n",
            "connoisseur\n",
            "profiterole\n",
            "outstanding\n",
            "acknowledged\n",
            "ventilation\n",
            "beautifully\n",
            "establishment\n",
            "extraordinary\n",
            "disappointed\n",
            "cheesecurds\n",
            "disappointed\n",
            "interesting\n",
            "experienced\n",
            "opportunity\n",
            "disgraceful\n",
            "restaurants\n",
            "ESTABLISHMENT\n",
            "recommended\n",
            "disappointed\n",
            "recommended\n",
            "acknowledged\n",
            "presentation\n",
            "Philadelphia\n",
            "disappointed\n",
            "disappointing\n",
            "grandmother\n",
            "drastically\n",
            "informative\n",
            "Disappointed\n",
            "constructed\n",
            "comfortable\n",
            "Smashburger\n",
            "cheeseburger\n",
            "neighborhood\n",
            "disappointed\n",
            "hospitality\n",
            "recommending\n",
            "disappointed\n",
            "deliciously\n",
            "compliments\n",
            "recommendation\n",
            "establishment\n",
            "calligraphy\n",
            "traditional\n",
            "combination\n",
            "Unfortunately\n",
            "Wienerschnitzel\n",
            "unfortunately\n",
            "considering\n",
            "highlighted\n",
            "Mediterranean\n",
            "unprofessional\n",
            "anticipated\n",
            "disappointing\n",
            "unexperienced\n",
            "disrespected\n",
            "professional\n",
            "restaurants\n",
            "Disappointing\n",
            "WAAAAAAyyyyyyyyyy\n",
            "reservation\n",
            "imagination\n",
            "undercooked\n",
            "disappointed\n",
            "disappointment\n",
            "disappointment\n",
            "deuchebaggery\n",
            "disappointed\n",
            "disappointment\n",
            "immediately\n",
            "Unfortunately\n",
            "disapppointment\n",
            "circumstances\n",
            "undercooked\n",
            "caterpillar\n",
            "presentation\n",
            "disappointed\n",
            "underwhelming\n",
            "\n",
            "Número total de palabras con más de 10 letras: 141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 10.**  \n",
        "\n",
        "Busca e imprime todas las palabras que inician con una letra mayúscula y terminan con una minúscula, pero que además no sea la primera palabra del comentario/string.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "ApjTNzSxMpDc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = []\n",
        "\n",
        "# Recorrer cada comentario\n",
        "for comentario in docs_limpios:\n",
        "    # Dividir en palabras\n",
        "    palabras = comentario.split()\n",
        "\n",
        "    # Omitir la primera palabra\n",
        "    for palabra in palabras[1:]:\n",
        "        # Verificar que empiece con mayúscula y termine con minúscula\n",
        "        if len(palabra) >= 2 and palabra[0].isupper() and palabra[-1].islower():\n",
        "            resultados.append(palabra)"
      ],
      "metadata": {
        "id": "Vb0ndRGAMqdL"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir resultados\n",
        "for palabra in resultados:\n",
        "    print(palabra)\n",
        "\n",
        "print(f\"\\nNúmero total de palabras que cumplen la condición: {len(resultados)}\")"
      ],
      "metadata": {
        "id": "dLPTRPnTMqqx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cef1be8-ad16-4e67-edd9-3b7896fd2168"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loved\n",
            "May\n",
            "Rick\n",
            "Steve\n",
            "Cape\n",
            "Cod\n",
            "Burrittos\n",
            "The\n",
            "They\n",
            "Mexican\n",
            "Luke\n",
            "Our\n",
            "Hiro\n",
            "Greek\n",
            "Greek\n",
            "Heart\n",
            "Attack\n",
            "Grill\n",
            "Vegas\n",
            "Dos\n",
            "Jeff\n",
            "Very\n",
            "Bad\n",
            "Customer\n",
            "Service\n",
            "I've\n",
            "Vegas\n",
            "Rice\n",
            "Pho\n",
            "Hard\n",
            "Rock\n",
            "Casino\n",
            "Buffet\n",
            "Tigerlilly\n",
            "Yama\n",
            "Thai\n",
            "Indian\n",
            "Not\n",
            "I'll\n",
            "I'm\n",
            "Lox\n",
            "Subway\n",
            "Mandalay\n",
            "Great\n",
            "Voodoo\n",
            "I'd\n",
            "Phoenix\n",
            "Vegas\n",
            "Khao\n",
            "Soi\n",
            "I'm\n",
            "Lemon\n",
            "Joey's\n",
            "Valley\n",
            "Phoenix\n",
            "Fridays\n",
            "I've\n",
            "Jamaican\n",
            "Bussell\n",
            "Filet\n",
            "Otto\n",
            "M's\n",
            "Not\n",
            "Greek\n",
            "Veggitarian\n",
            "Madison\n",
            "Jenni\n",
            "Bachi\n",
            "Burger\n",
            "Pizza\n",
            "They\n",
            "Bachi\n",
            "I'd\n",
            "English\n",
            "I've\n",
            "I've\n",
            "Pizza\n",
            "Hut\n",
            "Seat\n",
            "I've\n",
            "Vegas.....there\n",
            "Gordon\n",
            "Ramsey's\n",
            "Steak\n",
            "I've\n",
            "Outstanding\n",
            "Best\n",
            "Food\n",
            "I'll\n",
            "Lobster\n",
            "Bisque\n",
            "Eggplant\n",
            "Green\n",
            "Bean\n",
            "I'm\n",
            "Vegas\n",
            "Vegas\n",
            "Crystals\n",
            "Ians\n",
            "I'm\n",
            "San\n",
            "Francisco\n",
            "Bay\n",
            "Buldogis\n",
            "Gourmet\n",
            "Hot\n",
            "Dog\n",
            "I'm\n",
            "I've\n",
            "Steiners\n",
            "I'll\n",
            "Carly's\n",
            "Vegas\n",
            "Camelback\n",
            "Flower\n",
            "Shop\n",
            "Cartel\n",
            "Las\n",
            "I've\n",
            "Very\n",
            "Mom's\n",
            "Vegas\n",
            "Mexican\n",
            "I'm\n",
            "Perfect\n",
            "Vegas\n",
            "This\n",
            "I'm\n",
            "Thai\n",
            "I'll\n",
            "Thai\n",
            "Crema\n",
            "Café\n",
            "North\n",
            "Bloody\n",
            "Pho\n",
            "Caesar\n",
            "Macarons\n",
            "Very\n",
            "Disappointed\n",
            "Big\n",
            "Bay\n",
            "Italian\n",
            "Baba\n",
            "Ganoush\n",
            "Smashburger\n",
            "I've\n",
            "I'm\n",
            "Panna\n",
            "Cotta\n",
            "Mango\n",
            "Pineapple\n",
            "Delight\n",
            "I've\n",
            "The\n",
            "Strip\n",
            "Paradise\n",
            "Valley\n",
            "Cibo\n",
            "Thumbs\n",
            "Italian\n",
            "Pros\n",
            "Large\n",
            "Nice\n",
            "Great\n",
            "The\n",
            "Elk\n",
            "Filet\n",
            "Dylan\n",
            "All\n",
            "Han\n",
            "Nan\n",
            "Chicken\n",
            "Bar\n",
            "Edinburgh\n",
            "Chinese\n",
            "I've\n",
            "Indian\n",
            "Chinese\n",
            "I'm\n",
            "I'll\n",
            "Prices\n",
            "Phoenix\n",
            "Both\n",
            "Hot\n",
            "Sour\n",
            "Egg\n",
            "Flower\n",
            "Soups\n",
            "Sunday\n",
            "Hunan\n",
            "The\n",
            "I'm\n",
            "Pita\n",
            "Wienerschnitzel\n",
            "Maine\n",
            "Lobster\n",
            "Roll\n",
            "I'm\n",
            "Kabuki\n",
            "Maria\n",
            "Caballero's\n",
            "I'm\n",
            "Wife\n",
            "I've\n",
            "I've\n",
            "I'm\n",
            "To\n",
            "Place\n",
            "Japanese\n",
            "Albondigas\n",
            "Mediterranean\n",
            "Chicken\n",
            "Salad\n",
            "Mellow\n",
            "Thai\n",
            "Vegas\n",
            "Buffet\n",
            "Bellagio\n",
            "Vegas\n",
            "Christmas\n",
            "Eve\n",
            "Vegetarian\n",
            "I've\n",
            "Taco\n",
            "Heimer\n",
            "Ha\n",
            "Long\n",
            "Bay\n",
            "Subway\n",
            "When\n",
            "In\n",
            "Ninja\n",
            "Sushi\n",
            "\n",
            "Número total de palabras que cumplen la condición: 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 11.**  \n",
        "\n",
        "Busca e imprime la secuencia de dos o más palabras que están separadas por un guion, \"-\", sin que tengan espacios en blanco entre ellas.\n",
        "\n",
        "Por ejemplo \"Go-Kart\" sería válido, pero \"Go  -Kart\" o \"Go  -  Kart\" no lo serían.\n",
        "\n",
        "Indica la cantidad de resultados obtenidos."
      ],
      "metadata": {
        "id": "u7nfm4KhMrNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Expresión regular para encontrar secuencias de palabras separadas por un guion sin espacios\n",
        "pattern = r'\\b\\w+-\\w+\\b'\n",
        "\n",
        "# Buscar las coincidencias\n",
        "resultados = re.findall(pattern, texto)"
      ],
      "metadata": {
        "id": "OwU-a7eGMsub"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los resultados\n",
        "for resultado in resultados:\n",
        "    print(resultado)\n",
        "\n",
        "# Mostrar cuántos resultados se encontraron\n",
        "print(f\"\\nNúmero total de secuencias con guion: {len(resultados)}\")"
      ],
      "metadata": {
        "id": "SgzIL74ZMtGw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ad6c847-ff78-41cd-8e78-d636b86aeb6b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flat-lined\n",
            "hands-down\n",
            "must-stop\n",
            "sub-par\n",
            "Service-check\n",
            "in-house\n",
            "been-stepped\n",
            "in-and\n",
            "tracked-everywhere\n",
            "multi-grain\n",
            "to-go\n",
            "non-customer\n",
            "High-quality\n",
            "sit-down\n",
            "over-whelm\n",
            "low-key\n",
            "non-fancy\n",
            "golden-crispy\n",
            "over-priced\n",
            "over-hip\n",
            "under-services\n",
            "\n",
            "Número total de secuencias con guion: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 12.**  \n",
        "\n",
        "Busca e imprime todas las palabras que terminan en \"ing\" o \"ed\".\n",
        "\n",
        "Indica la cantidad de palabras que encontraste de cada una."
      ],
      "metadata": {
        "id": "DEIgl79HMthr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unir todos los comentarios en un solo texto\n",
        "texto = ' '.join(docs_limpios)\n",
        "\n",
        "# Expresión regular para encontrar palabras que terminan en 'ing' o 'ed'\n",
        "pattern = r'\\b\\w+(ing|ed)\\b'\n",
        "\n",
        "# Buscar las coincidencias\n",
        "resultados = re.findall(pattern, texto)\n",
        "\n",
        "# Separar los resultados en dos listas: \"ing\" y \"ed\"\n",
        "terminan_en_ing = [palabra for palabra in resultados if palabra.endswith('ing')]\n",
        "terminan_en_ed = [palabra for palabra in resultados if palabra.endswith('ed')]"
      ],
      "metadata": {
        "id": "I4TSofBMMv9y"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los resultados\n",
        "print(\"Palabras que terminan en 'ing':\")\n",
        "for palabra in terminan_en_ing:\n",
        "    print(palabra)\n",
        "\n",
        "print(\"\\nPalabras que terminan en 'ed':\")\n",
        "for palabra in terminan_en_ed:\n",
        "    print(palabra)\n",
        "\n",
        "# Mostrar la cantidad de palabras encontradas\n",
        "print(f\"\\nNúmero total de palabras que terminan en 'ing': {len(terminan_en_ing)}\")\n",
        "print(f\"Número total de palabras que terminan en 'ed': {len(terminan_en_ed)}\")"
      ],
      "metadata": {
        "id": "AhGq6De2Mvyh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d13b4fa-ed0a-4d48-f52f-a63d82bd8d99"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Palabras que terminan en 'ing':\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "ing\n",
            "\n",
            "Palabras que terminan en 'ed':\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "ed\n",
            "\n",
            "Número total de palabras que terminan en 'ing': 279\n",
            "Número total de palabras que terminan en 'ed': 335\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Parte 3. Proceso de limpieza.**"
      ],
      "metadata": {
        "id": "70StdqAZa9E9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 13.**  \n",
        "\n",
        "Ahora realiza un proceso de limpieza del corpus que incluya los siguientes procesos:\n",
        "\n",
        "*   Solo se deben considerar caracteres alfabéticos. Es decir, se eliminan todos los signos de puntuación y caracteres especiales.\n",
        "*   Todos los caracteres alfabéticos se transforman a minúsculas.\n",
        "*   Se deben eliminar todos los espacios en blanco adicionales que se puedan encontrar en cada comentario.\n",
        "\n",
        "Al finalizar dicho proceso de limpieza, imprime el resultado de los primeros 10 comentarios resultantes.\n",
        "   \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xaDUFXHrMvX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para limpiar un comentario\n",
        "def limpiar_comentario(comentario):\n",
        "    # Eliminar caracteres no alfabéticos (solo letras) y reemplazarlo por espacio\n",
        "    comentario = re.sub(r'[^a-zA-ZáéíóúÁÉÍÓÚ]+', ' ', comentario)  # Reemplaza lo no alfabético por espacios\n",
        "    # Convertir a minúsculas\n",
        "    comentario = comentario.lower()\n",
        "    # Eliminar espacios adicionales (espacios extra entre palabras)\n",
        "    comentario = ' '.join(comentario.split())\n",
        "    return comentario\n",
        "\n",
        "# Limpiar todos los comentarios\n",
        "comentarios_limpios = [limpiar_comentario(comentario) for comentario in docs_limpios]"
      ],
      "metadata": {
        "id": "K3kQzPOPMx0w"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir los primeros 10 comentarios limpios\n",
        "for comentario in comentarios_limpios[:10]:\n",
        "    print(comentario)"
      ],
      "metadata": {
        "id": "mYEDlHSFMyJN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d488eb8-0cd4-4129-e5ac-06f9c38cf7a2"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wow loved this place\n",
            "crust is not good\n",
            "not tasty and the texture was just nasty\n",
            "stopped by during the late may bank holiday off rick steve recommendation and loved it\n",
            "the selection on the menu was great and so were the prices\n",
            "now i am getting angry and i want my damn pho\n",
            "honeslty it didn t taste that fresh\n",
            "the potatoes were like rubber and you could tell they had been made up ahead of time being kept under a warmer\n",
            "the fries were great too\n",
            "a great touch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 14.**  \n",
        "\n",
        "Con el resultado de la limpieza obtenido en la pregunta anterior, realiza ahora un proceso de tokenización por palabras del corpus.\n",
        "\n",
        "Es decir, al final de este proceso de tokenización, debes tener como resultado una lista de listas, donde cada comentario estará tokenizado por palabras.\n",
        "\n",
        "Al terminar calcula el total de tokens obtenido en todo el corpus."
      ],
      "metadata": {
        "id": "WZwEhg2lUSAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenización por palabras: dividir cada comentario en palabras\n",
        "comentarios_tokenizados = [comentario.split() for comentario in comentarios_limpios]\n",
        "\n",
        "# Imprimir los primeros 10 comentarios tokenizados\n",
        "for comentario_tokenizado in comentarios_tokenizados[:10]:\n",
        "    print(comentario_tokenizado)\n",
        "\n",
        "# Calcular el total de tokens (palabras) en el corpus\n",
        "total_tokens = sum(len(comentario) for comentario in comentarios_tokenizados)"
      ],
      "metadata": {
        "id": "kbAL9-v0V-jx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a07d82-9bf8-4c9a-ef38-493b29c818be"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wow', 'loved', 'this', 'place']\n",
            "['crust', 'is', 'not', 'good']\n",
            "['not', 'tasty', 'and', 'the', 'texture', 'was', 'just', 'nasty']\n",
            "['stopped', 'by', 'during', 'the', 'late', 'may', 'bank', 'holiday', 'off', 'rick', 'steve', 'recommendation', 'and', 'loved', 'it']\n",
            "['the', 'selection', 'on', 'the', 'menu', 'was', 'great', 'and', 'so', 'were', 'the', 'prices']\n",
            "['now', 'i', 'am', 'getting', 'angry', 'and', 'i', 'want', 'my', 'damn', 'pho']\n",
            "['honeslty', 'it', 'didn', 't', 'taste', 'that', 'fresh']\n",
            "['the', 'potatoes', 'were', 'like', 'rubber', 'and', 'you', 'could', 'tell', 'they', 'had', 'been', 'made', 'up', 'ahead', 'of', 'time', 'being', 'kept', 'under', 'a', 'warmer']\n",
            "['the', 'fries', 'were', 'great', 'too']\n",
            "['a', 'great', 'touch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar el total de tokens\n",
        "print(f\"\\nNúmero total de tokens en todo el corpus: {total_tokens}\")"
      ],
      "metadata": {
        "id": "DZs_etmiV-fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c96ab7b9-408b-476b-f1fe-206dac01f5d6"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número total de tokens en todo el corpus: 11039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Pregunta 15.**  \n",
        "\n",
        "Finalmente, en este ejercicio definiremos nuestro conjunto de palabras \"stopwords\", las cuales deberás eliminar de todo el corpus.\n",
        "\n",
        "Recuerda que ejemplos de stopwords son artículos, adverbios, conectivos, etcétera, que tienen frecuencias de aparición muy altas en cualquier documento, pero que no brindan mucho significado en cuanto al significado de un enunciado.\n",
        "\n",
        "Con base a la lista de stopwords que se te proporciona, realiza un proceso de limpieza eliminando todas estas palabras del corpus obtenido en el ejercicio anterior.\n",
        "\n",
        "Obtener cuántos tokens/palabras quedan finalmente en todo el corpus.\n",
        "\n",
        "Obtener cuántos de estos tokens/palabras son diferentes, es decir, cuántos tokens únicos tendrá lo que llamaremos más adelante nuestro vocabulario."
      ],
      "metadata": {
        "id": "EFeu0OJ7WDPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Considera la siguiente lista como tu conjunto de stopwords:\n",
        "mis_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'should', 'now', 'll']"
      ],
      "metadata": {
        "id": "6FP4FF3KXGxm"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar las stopwords de cada comentario tokenizado\n",
        "comentarios_sin_stopwords = [\n",
        "    [palabra for palabra in comentario if palabra not in mis_stopwords]\n",
        "    for comentario in comentarios_tokenizados\n",
        "]\n",
        "\n",
        "# Imprimir los primeros 10 comentarios sin stopwords\n",
        "for comentario_sin_stopwords in comentarios_sin_stopwords[:10]:\n",
        "    print(comentario_sin_stopwords)\n",
        "\n",
        "# Contar el total de tokens que quedan\n",
        "total_tokens_sin_stopwords = sum(len(comentario) for comentario in comentarios_sin_stopwords)\n",
        "\n",
        "# Contar el número de tokens únicos (vocabulario)\n",
        "vocabulario_unico = set([palabra for comentario in comentarios_sin_stopwords for palabra in comentario])\n",
        "tamaño_vocabulario = len(vocabulario_unico)"
      ],
      "metadata": {
        "id": "CD8yjyq1ZrwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07eeea7-aa2c-465d-ee1d-86e6f6aae31a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['wow', 'loved', 'place']\n",
            "['crust', 'not', 'good']\n",
            "['not', 'tasty', 'texture', 'nasty']\n",
            "['stopped', 'late', 'may', 'bank', 'holiday', 'off', 'rick', 'steve', 'recommendation', 'loved']\n",
            "['selection', 'menu', 'great', 'prices']\n",
            "['getting', 'angry', 'want', 'damn', 'pho']\n",
            "['honeslty', 'didn', 'taste', 'fresh']\n",
            "['potatoes', 'like', 'rubber', 'could', 'tell', 'made', 'ahead', 'time', 'kept', 'warmer']\n",
            "['fries', 'great']\n",
            "['great', 'touch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mostrar los resultados\n",
        "print(f\"\\nNúmero total de tokens después de eliminar las stopwords: {total_tokens_sin_stopwords}\")\n",
        "print(f\"Total de tokens únicos en el vocabulario: {tamaño_vocabulario}\")"
      ],
      "metadata": {
        "id": "4ZPi5prKZro5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eaf39c1-1c27-47ab-c68d-1512f98763db"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Número total de tokens después de eliminar las stopwords: 5783\n",
            "Total de tokens únicos en el vocabulario: 1910\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Comentarios**\n",
        "\n",
        "Anteriormente se realizó un análisis de texto utilizando técnicas de procesamiento de lenguaje natural, se procesó el archivo cargado, realizando limpieza (eliminar signos de puntuación, convertir todo a minúsculas y eliminar espacios adicionales) y preparación (tokenización del texto en palabras, lo cual es una parte fundamental en el análisis de texto, ya que permite tratar cada palabra individualmente)\n",
        "\n",
        "Además se realizó la eliminación de stopword (palabras que tienen alta frecuencia pero bajo valor semántico como artículos, preposiciones, etc.), lo cual es crucial para reducir el ruido en el análisis de texto y hacer que los modelos se enfoquen en las palabras más relevantes.\n",
        "\n",
        "Conclusión\n",
        "\n",
        "Se aplicaron lo principios fundamentales de procesamiento de texto, que son cruciales cuando se trabaja con grandes volúmenes de datos, la tokenización y la eliminación de stopwords son pasos básicos pero esenciales para preparar los datos para tareas de análisis o modelado."
      ],
      "metadata": {
        "id": "NDbKkuxRbLoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Fin de la Actividad de la semana 2.**"
      ],
      "metadata": {
        "id": "PHaKw_6Ldbaf"
      }
    }
  ]
}